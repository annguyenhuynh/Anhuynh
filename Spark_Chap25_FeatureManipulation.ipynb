{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nhAx5q0fc3MroDKBCH18NOzZh-LMIuDR",
      "authorship_tag": "ABX9TyN0xwuMCPNyB1dGc5PR5aaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annguyenhuynh/Anhuynh/blob/main/Spark_Chap25_FeatureManipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63jWH8epoyjg",
        "outputId": "5e121c5c-bacd-42a3-b7fc-7882f11465ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spark\n",
            "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: spark\n",
            "  Building wheel for spark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58748 sha256=1bf4f8d7c8c7b093a8dfd66be2aee717c332dea1aa98063b01743d5042995568\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/88/77/b4131110ea4094540f7b47c6d62a649807d7e94800da5eab0b\n",
            "Successfully built spark\n",
            "Installing collected packages: spark\n",
            "Successfully installed spark-0.2.1\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=487f8fa2ea9d5bae12ba2dcfb391d3faca0411d0a8a723eb30ea70c6a69fa36a\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install spark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "vPyOptF8o8Ie"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_PXks0ZkybN",
        "outputId": "ed64266d-ed55-4076-a996-f1008c0cbcb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "YG29qtweo_1J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales = spark.read.format(\"csv\")\\\n",
        "  .option(\"header\",True)\\\n",
        "  .option(\"inferSchema\",True)\\\n",
        "  .load(\"/content/drive/MyDrive/bestModel/metadata/databricks Spark-The-Definitive-Guide master data-retail-data_by-day/*.csv\")\\\n",
        "  .coalesce(5)\\\n",
        "  .where(\"Description is NOT NULL\")\n",
        "\n",
        "fakeInDF = spark.read.parquet(\"/content/drive/MyDrive/databricks Spark-The-Definitive-Guide master data-simple-ml-integers\")\n",
        "\n",
        "simpleDF = spark.read.json(\"/content/drive/MyDrive/databricks Spark-The-Definitive-Guide master data-simple-ml\")\n",
        "\n",
        "scaledDF = spark.read.parquet(\"/content/drive/MyDrive/databricks Spark-The-Definitive-Guide master data-simple-ml-scaling\")"
      ],
      "metadata": {
        "id": "kw_I5amYpN8y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales.cache()\n",
        "sales.show()"
      ],
      "metadata": {
        "id": "8YMixBIMp0Oy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f021e8-cca5-47d7-a511-241833ee20ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
            "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
            "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
            "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
            "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
            "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
            "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
            "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
            "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
            "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
            "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
            "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
            "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
            "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
            "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
            "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
            "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RFormula\n",
        "\n",
        "supervised = RFormula(formula= \"lab ~ . + color:value1 + color:value2\")\n",
        "supervised.fit(simpleDF).transform(simpleDF).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3rlIzwQqScQ",
        "outputId": "3293b580-ac71-4e8b-e0a1-3c9c68f91ebb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+------------------+--------------------+-----+\n",
            "|color| lab|value1|            value2|            features|label|\n",
            "+-----+----+------+------------------+--------------------+-----+\n",
            "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
            "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
            "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
            "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
            "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
            "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
            "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
            "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
            "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
            "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
            "|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n",
            "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
            "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
            "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
            "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
            "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
            "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
            "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
            "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
            "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
            "+-----+----+------+------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler"
      ],
      "metadata": {
        "id": "WacEd3LRqpkd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sScaler = StandardScaler().setInputCol(\"features\")\n",
        "sScaler.fit(scaledDF).transform(scaledDF).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIph4VPequMh",
        "outputId": "34e3f690-adc1-4316-942a-36ae0f929654"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+-----------------------------------+\n",
            "| id|      features|StandardScaler_bdf85cc89f90__output|\n",
            "+---+--------------+-----------------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
            "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
            "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
            "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
            "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
            "+---+--------------+-----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA** - PRINCIPAL COMPONENTS ANALYSIS\n",
        "\n",
        "*   A mathematical technique for finding the most imoortant aspects of our data (the principal components)\n",
        "*   It changes the feature representation of our data by creating a new set of features(\"aspects\"). Each new feature is a combination of the original features.\n",
        "*   The power of PCA is that it can create a **SMALLER** set of more **MEANINGFUL** feaures to be input of our mode, at the potential cost of our interpretability.\n",
        "*   We want to use PCA when we have large input dataset and want to reduce the total number of features. This frequently comes up in text analysis where the entire feature space is massive and many of the features are largely irrelevant.\n",
        "*   Using PCA, we can find the most important combinations if features and only include those in our ML models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EK7NcywFrexj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import PCA"
      ],
      "metadata": {
        "id": "iMjCcMrUrCnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA().setInputCol('features').setK(2)\n",
        "pca.fit(scaledDF).transform(scaledDF).show(40,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqboY4FBtRzi",
        "outputId": "f518d911-03ca-447c-e94d-eff772e7fd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+------------------------------------------+\n",
            "|id |features      |PCA_95df978c2243__output                  |\n",
            "+---+--------------+------------------------------------------+\n",
            "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.45266548881478363] |\n",
            "|1  |[2.0,1.1,1.0] |[-1.680494698407372,1.259340132221916]    |\n",
            "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.45266548881478363] |\n",
            "|1  |[2.0,1.1,1.0] |[-1.680494698407372,1.259340132221916]    |\n",
            "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060152422]|\n",
            "+---+--------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preserving the Variance\n",
        "*   Select the hyperplane that preserves the **maximum** amount of varianace, as it will most likely lose less information than other projections\n",
        "*   If you can plot them on a 2D hyperplane, you should choose the axis that **minimized the mean-squared distance** between the original dataset and its projection on that axis.\n",
        "\n"
      ],
      "metadata": {
        "id": "lNF2NzqngPBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Components\n",
        "\n",
        "*   PCA identifies the axis that accounts for the largest amount of variance\n",
        "\n",
        "*   It also finds the 2nd axis, orthogonal to the first one, that accounts for the largest amount of remaining variance\n",
        "\n",
        "*   The ith axis is called the ith *princial component (PC)** of the data\n",
        "\n",
        "*   For each PC, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since 2 opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable.\n",
        "\n",
        "*   To find PCs, use a **standard matrix factorization** called ***Singular Value Decomposition (SVD)*** that can decompose the training set matrix X into the matrix multiplication of 3 matrices\n",
        "U S Vt, where V contains the unit vectors that define all the principal components that we are looking for\n",
        "\n",
        "*   Before implemeting PCA, ensure that data is centered around the origin (zero-centered) by performing data normalizing and scaling.\n",
        "    \n",
        "                  *X_centered = X-X.mean(axis=0)*\n",
        "                  *U,S,Vt = np.linagl.svd(X_centered)*\n",
        "\n",
        "*  In Python, use **sklearn.decompostion.PCA** and set *n_components* to the number of dimensions you want to reduce to.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZvDgmCYChZd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaledDF.show(10,False)"
      ],
      "metadata": {
        "id": "YjTJseZVtg5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170f27ab-8466-48e6-838f-8daf4fe422d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+\n",
            "|id |features      |\n",
            "+---+--------------+\n",
            "|0  |[1.0,0.1,-1.0]|\n",
            "|1  |[2.0,1.1,1.0] |\n",
            "|0  |[1.0,0.1,-1.0]|\n",
            "|1  |[2.0,1.1,1.0] |\n",
            "|1  |[3.0,10.1,3.0]|\n",
            "+---+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "NKbuyZ_xo0SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "scaled_data = np.array(scaledDF.select(\"features\").collect())\n",
        "scaled_data = scaled_data.reshape(scaled_data.shape[0],-1)\n",
        "X_reduced = pca.fit_transform(scaled_data)"
      ],
      "metadata": {
        "id": "Fa6N9Sjstu3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_reduced"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-y9lfFTuCkK",
        "outputId": "1bbefa97-1772-4a26-f76f-b9fcfac0efad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.88950068],\n",
              "       [-1.13763403],\n",
              "       [-2.88950068],\n",
              "       [-1.13763403],\n",
              "       [ 8.05426941]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   In the above code, instead of specifying the number of principal components to be reserved, we set n_components to a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DDbbjYogvC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Math intuition behind PCA\n",
        "\n",
        "\n",
        "*   The goal of PCA is to find the unit vector that captures the max variance.\n",
        "\n",
        "*   The technique to select the best unit vector is called EigenDecomposition. Eigendecomposition breaks down a matrix into its eigenvalues and eigenvectors to help you find the non-obvious and universal properties.\n",
        "\n",
        "*   In linear algebra, an eigenvector, or characteristic vector is a vector that has its direction unchanged by a given linear transformation. More precisely, an eigenvector,\n",
        "v of a linear transformation, T is scaled by a constant factor, λ when the linear transformation is applied to it:  **Tv = λv**. It is often important to know these vectors in linear algebra. The corresponding eigenvalue, characteristic value, or characteristic root is the multiplying factor\n",
        "λ. (Wikipedia)\n",
        "\n",
        "*   Geometrically, vectors are multi-dimensional quantities with magnitude and direction, often pictured as arrows. A linear transformation rotates, stretches, or shears the vectors upon which it acts. Its eigenvectors are those vectors that are only stretched, with neither rotation nor shear. The corresponding eigenvalue is the factor by which an eigenvector is stretched or squished. If the eigenvalue is negative, the eigenvector's direction is reversed (Wikipedia)\n",
        "\n"
      ],
      "metadata": {
        "id": "x-paD5ue2S-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interaction\n",
        "\n",
        "*   In some cases, you might know that a certain interaction between 2 variables is an important varialble to include in a downstream estimator.\n",
        "\n",
        "*   RFormula: (y ~ x1+x2+x1:x2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yfH2asrXlWTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Expansion\n",
        "\n",
        "*   Used to generate interaction variables of all the input columns. With polynomial expansions, we specify to what degree we would like to see various interactions.\n",
        "\n",
        "*   If we have 2 input features, we'll get 4 (2x2) output features if we use 2nd degree polynomial. If we have 3 input features, we'll get 9 (3x3) output features; if we use 3rd degree polynomial, we will get 27(3*3*3) and so on.\n"
      ],
      "metadata": {
        "id": "-W9X21mA3qno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import PolynomialExpansion"
      ],
      "metadata": {
        "id": "o8yrIHY6u89F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
        "pe.transform(scaledDF).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX15LK9H5imb",
        "outputId": "1ca5ce0e-c22d-4656-c0f2-734a03e38b44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+----------------------------------------+\n",
            "| id|      features|PolynomialExpansion_4baf03b3fb6d__output|\n",
            "+---+--------------+----------------------------------------+\n",
            "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
            "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
            "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
            "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
            "|  1|[3.0,10.1,3.0]|                    [3.0,9.0,10.1,30....|\n",
            "+---+--------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import ChiSqSelector, Tokenizer"
      ],
      "metadata": {
        "id": "agL4ecjoLdrX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "4ymeDCp_Lk_I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
        "tokenized = tkn\\\n",
        "  .transform(sales.select(\"Description\",\"CustomerId\"))\\\n",
        "  .where(\"CustomerId is NOT NULL\")\n",
        "prechi = fittedCV.transform(tokenized)\\\n",
        "  .where(\"CustomerId is NOT NULL\")\n",
        "chisq = ChiSqSelector()\\\n",
        "  .setFeaturesCol(\"countVec\")\\\n",
        "  .setLabelCol(\"CustomerId\")\\\n",
        "  .setNumTopFeatures(2)\n",
        "chisq.fit(prechi).transform(prechi)\\\n",
        "  .drop(\"CustomerId\", \"Description\", \"DescOut\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9_LSe9FcJ7d6",
        "outputId": "798501bf-ac95-44aa-f20d-16388a3d0137"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fittedCV' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5fc0e3de436d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"CustomerId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CustomerId is NOT NULL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprechi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfittedCV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CustomerId is NOT NULL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mchisq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChiSqSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fittedCV' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChiSqSelector\n",
        "\n",
        "*   ChiSqSelector leverages statistical test to identify features that are not independent from the label we are trying to predict, and drop the uncorrelated features.\n",
        "\n",
        "*   It's often used with categorical data to reduce the number of features you will input into your model, as well as to reduce the dimensionality of text data (in the form of frequencies or counts)\n",
        "\n",
        "*   We can use **numTopFeatures**, which is ordered by p-values\n",
        "*   or **percentile**, which takes a proportion of the input features (instead of just the top N values)\n",
        "*   and **fpr** which sets the cutoff p-values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RRGJo5rq-odx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cPiR1fhW-XrP"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}